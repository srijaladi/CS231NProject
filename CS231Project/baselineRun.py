import numpy as npimport torchimport torch.nn as nn#import cv2import osfrom PIL import Image as imfrom matplotlib import pyplot as pltIN_H, IN_W = 128, 128IM_DIM = (3, IN_H, IN_W)DATA_FOLDER  = "DataFolder/"FRAME_FOLDER = DATA_FOLDER + "Frames/"LOG_FOLDER   = DATA_FOLDER + "Logging/"EXAMPLES_FOLDER = DATA_FOLDER + "Examples/"FRAME_DIFF = 5TRAIN_ITERATIONS = 1000BATCH_SIZE = 64ACC_CUTOFF = 100BATCH_DIM = (BATCH_SIZE, *IM_DIM)def load_frame_data(folderpath):    frames_list = sorted(os.listdir(folderpath))    num_frames  = len(frames_list)    all_frames  = np.zeros((num_frames, *IM_DIM))    for idx, frame in enumerate(frames_list):        filepath = folderpath + "/" + str(frame)        curr_frame = np.loadtxt(filepath)        curr_frame = np.reshape(curr_frame, IM_DIM)        all_frames[idx] = curr_frame    all_frames = all_frames[5:]    return torch.from_numpy(all_frames).float()def gen_batch_sample(data, batch_size, frame_diff : int):    l = int(len(data))    b_idxs = torch.randint(0,l-frame_diff-frame_diff, size = (batch_size,))    a_idxs = b_idxs + frame_diff + frame_diff    t_idxs = b_idxs + frame_diff        before_batch = data[b_idxs]    after_batch  = data[a_idxs]    target_batch = data[t_idxs]        return (before_batch, after_batch, target_batch)def gen_batch_pure(data, frame_diff: int):    l = int(len(data))    b_idxs = torch.arange(l - frame_diff - frame_diff).long()    a_idxs = b_idxs + frame_diff + frame_diff    t_idxs = b_idxs + frame_diff        before_batch = data[b_idxs]    after_batch  = data[a_idxs]    target_batch = data[t_idxs]        return (before_batch, after_batch, target_batch)def log_vals(vals : dict):    for key, val in vals.items():        filepath = LOG_FOLDER + key + ".txt"        np.savetxt(filepath, np.array(val))        def conv_image(image):    np_img = np.swapaxes(np.swapaxes(image.detach().numpy(), 0, 2), 0, 1)    return np_imgdef save_image(filepath, image):    np_img = conv_image(image)    plt.clf()    plt.imshow(np_img/255, interpolation='nearest')    plt.savefig(filepath)    plt.clf()        def save_images(folderpath, input_images, output_image):    if not(os.path.exists(folderpath)): os.mkdir(folderpath)    before_image, after_image, target_image = input_images        save_image(folderpath + "/before.png", before_image)    save_image(folderpath + "/after.png",  after_image)    save_image(folderpath + "/target.png", target_image)    save_image(folderpath + "/output.png",  output_image)    def save_examples(mode, model, batch):    assert mode == "val" or mode == "test" or mode == "train"    loss_per = model.eval_loss_per(*batch)    idxs = torch.argsort(loss_per)    for i in range(5):        idx = idxs[i]        x_1, x_2, t_x = batch[0][idx], batch[1][idx], batch[2][idx]        out_x = model(x_1, x_2)[0]        folderpath = EXAMPLES_FOLDER + mode + "_best_" + str(i+1)        save_images(folderpath, (x_1, x_2, t_x), out_x)            for i in range(1,6):        idx = idxs[-i]        x_1, x_2, t_x = batch[0][idx], batch[1][idx], batch[2][idx]        out_x = model(x_1, x_2)[0]        folderpath = EXAMPLES_FOLDER + mode + "_worst_" + str(i)        save_images(folderpath, (x_1, x_2, t_x), out_x)    class FrameGenModel(nn.Module):    def __init__(self, in_height, in_width, feature_layers, combine_layers, generation_layers, loss_function, learning_rates: dict):        super().__init__()        self.in_h  = in_height        self.in_w  = in_width        self.loss_func = loss_function                self.featureModel = torch.nn.Sequential(*feature_layers)        self.combineModel = torch.nn.Sequential(*combine_layers)        self.generationModel = torch.nn.Sequential(*generation_layers)                self.feature_optimizer = torch.optim.Adam(self.featureModel.parameters(), lr=learning_rates['features'])        self.combine_optimizer = torch.optim.Adam(self.combineModel.parameters(), lr=learning_rates['combine'])        #self.generation_optimizer = torch.optim.Adam(self.generationModel.parameters(), lr=learning_rates['generation'])            def make_batch(self, x):        if len(x.size()) == 3:            x = x.unsqueeze(0)        return x            def features_forward(self, in_x):        #print(in_x)        in_x = self.make_batch(in_x)        features_x = self.featureModel(in_x)        return features_x        def combine_forward(self, features_x):        features_x = self.make_batch(features_x)        combine_x = self.combineModel(features_x)        return combine_x        def generation_forward(self, combine_x):        combine_x = self.make_batch(combine_x)        image_out_x = self.generationModel(combine_x)        return image_out_x        def forward(self, in_x_1, in_x_2):        in_x_1 = self.make_batch(in_x_1)        in_x_2 = self.make_batch(in_x_2)        features_x_1  = self.features_forward(in_x_1)        features_x_2  = self.features_forward(in_x_2)        features_x    = torch.cat((features_x_1, features_x_2), dim = 1)        combine_x     = self.combine_forward(features_x)        image_out_x   = self.generation_forward(combine_x)        return image_out_x        def backprop(self, in_x_1, in_x_2, out_x):        in_x_1 = self.make_batch(in_x_1)        in_x_2 = self.make_batch(in_x_2)        out_x  = self.make_batch(out_x)                    self.feature_optimizer.zero_grad()        self.combine_optimizer.zero_grad()        #self.generation_optimizer.zero_grad()                image_out_x = self.forward(in_x_1, in_x_2)        loss = self.loss_func(image_out_x, out_x)        loss.backward()                self.feature_optimizer.step()        self.combine_optimizer.step()        #self.generation_optimizer.step()                return loss.item()        def eval_loss_total(self, in_x_1, in_x_2, out_x):        in_x_1 = self.make_batch(in_x_1)        in_x_2 = self.make_batch(in_x_2)        out_x  = self.make_batch(out_x)                image_out_x = self.forward(in_x_1, in_x_2)        loss = self.loss_func(image_out_x, out_x)                return loss.item()        def eval_loss_per(self, in_x_1, in_x_2, out_x):        in_x_1 = self.make_batch(in_x_1)        in_x_2 = self.make_batch(in_x_2)        out_x  = self.make_batch(out_x)                image_out_x = self.forward(in_x_1, in_x_2)        loss_per = torch.mean(torch.square(image_out_x - out_x), dim = (1,2,3))                if len(loss_per.size()) > 1: loss_per.squeeze(1)        assert len(loss_per.size()) == 1        return loss_per            def eval_acc(self, in_x_1, in_x_2, out_x, cutoff):        in_x_1 = self.make_batch(in_x_1)        in_x_2 = self.make_batch(in_x_2)        out_x  = self.make_batch(out_x)                loss_per = self.eval_loss_per(in_x_1, in_x_2, out_x)        acc_per  = torch.minimum(cutoff/loss_per, torch.zeros_like(loss_per) + 1)        return float(torch.mean(acc_per))        k_size = (5,5)in_h, in_w, feature_size = IN_H, IN_W, 32"""feature_layers = [torch.nn.Conv2d(3, 4, k_size, padding = 'same'), torch.nn.MaxPool2d((2,2), stride = 2), torch.nn.ReLU(),                   torch.nn.Conv2d(4, 2, k_size, padding = 'same'), torch.nn.MaxPool2d((2,2), stride = 2), torch.nn.ReLU(),                   torch.nn.Conv2d(2, 1, k_size, padding = 'same'), torch.nn.Flatten(), torch.nn.Linear(1 * 32 * 32, feature_size)]generation_layers = [torch.nn.Linear(feature_size * 2, feature_size * feature_size), torch.nn.Unflatten(1, (1,feature_size,feature_size)), torch.nn.ReLU(),                      torch.nn.ConvTranspose2d(1, 2, (4,4), stride=2, padding = 1), torch.nn.ReLU(),                      torch.nn.ConvTranspose2d(2, 3, (4,4), stride=2, padding = 1), torch.nn.ReLU()]test = [torch.nn.Conv2d(3, 4, k_size, padding = 'same'), torch.nn.MaxPool2d((2,2), stride = 2), torch.nn.ReLU(), torch.nn.Flatten()]"""fl = [torch.nn.Conv2d(3, 2, k_size, padding = 'same'), torch.nn.MaxPool2d((2,2), stride = 2), torch.nn.ReLU(),       torch.nn.Conv2d(2, 1, k_size, padding = 'same'), torch.nn.MaxPool2d((2,2), stride = 2), torch.nn.ReLU(),      torch.nn.Flatten(), torch.nn.Linear(32*32,64)]cl = [torch.nn.Linear(128,128*128*3)]gl = [torch.nn.Unflatten(1, (3,128,128))]learning_rates = {'features':1e-3, 'combine':1e-3, 'generation':1e-3}#baselineModel = FrameGenModel(in_h, in_w, feature_layers, generation_layers, torch.nn.functional.mse_loss, learning_rates)baselineModel = FrameGenModel(in_h, in_w, fl, cl, gl, torch.nn.functional.mse_loss, learning_rates)print("MODEL CREATED")baseline_video_data = data = load_frame_data(FRAME_FOLDER + "test_video_frames")print("LOADED DATA")data_amt = int(len(baseline_video_data))train_amt = int(data_amt * 0.8)val_amt = int(data_amt * 0.1)test_amt = int(data_amt - train_amt - val_amt)train_data = data[:train_amt]val_data   = data[train_amt:train_amt + val_amt]test_data  = data[-test_amt:]val_batch  = gen_batch_pure(val_data, FRAME_DIFF)test_batch = gen_batch_pure(test_data, FRAME_DIFF)print("CREATED TESTING AND VAL BATCHES")log = {'train_losses' : [], 'val_losses' : [], 'val_accuracies' : []}    for itr in range(TRAIN_ITERATIONS):    train_batch = gen_batch_sample(train_data, BATCH_SIZE, FRAME_DIFF)    train_loss  = baselineModel.backprop(*train_batch)    val_acc     = baselineModel.eval_acc(*val_batch, ACC_CUTOFF)    val_loss    = baselineModel.eval_loss_total(*val_batch)        log['train_losses'].append(train_loss)    log['val_losses'].append(val_loss)    log['val_accuracies'].append(val_acc)        log_vals(log)        if itr%10 == 0:        print("ITERATION: " + str(itr))        print("TRAIN LOSS: " + str(train_loss))        print("VAL ACCURACY: " + str(val_acc))        print("VAL LOSS: " + str(val_loss))        save_examples("val", baselineModel, val_batch)test_acc = baselineModel.eval_acc(*test_batch, ACC_CUTOFF)test_loss = baselineModel.eval_loss_total(*test_batch)print("LOSS ON TEST SET: " + str(test_loss))print("ACCURACY ON TEST SET: " + str(test_acc))save_examples("test", baselineModel, test_batch)"""before_frame = np.reshape(np.loadtxt("DataFolder/Frames/test_video_frames/frame_51.txt"), (3,in_h,in_w))after_frame = np.reshape(np.loadtxt("DataFolder/Frames/test_video_frames/frame_55.txt"), (3,in_h,in_w))target_frame = np.reshape(np.loadtxt("DataFolder/Frames/test_video_frames/frame_60.txt"), (3,in_h,in_w))bf = torch.from_numpy(before_frame).float()af = torch.from_numpy(after_frame).float()tf = torch.from_numpy(target_frame).float()for itr in range(100):    loss = baselineModel.backprop(bf, af, tf)    if itr%100 == 0:        print(loss)res_f = baselineModel(bf, af)[0]#print(res_f.size())res_f_np = res_f.detach().numpy()print(res_f_np)#print(res_f_np.shape)res_for_im = np.swapaxes(np.swapaxes(res_f_np, 0, 2), 0, 1)plt.imshow(res_for_im/255, interpolation='nearest')plt.savefig("out100.png")tf_tt = np.swapaxes(np.swapaxes(target_frame, 0, 2), 0, 1)plt.imshow(tf_tt/255, interpolation='nearest')plt.savefig("target.png")#print(res_for_im.shape)#pic = im.fromarray(res_for_im)#pic.save("testimg.png")"""