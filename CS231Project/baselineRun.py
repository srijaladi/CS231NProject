import numpy as npimport torchimport torch.nn as nn#import cv2import osfrom PIL import Image as imfrom matplotlib import pyplot as pltimport skimagefrom skimage.measure import block_reduce as AVG_POOLDEVICE = "cuda" if torch.cuda.is_available() else "cpu"print(DEVICE)TRUE_C, TRUE_H, TRUE_W= 3, 128, 128 IN_C, IN_H, IN_W = 3, 32, 32IN_DIM = (IN_C, IN_H, IN_W)TRUE_DIM = (TRUE_C, TRUE_H, TRUE_W)BATCH_TEST_DIM = (1, IN_C, IN_H, IN_W)POOL_METHOD = np.meanPOOL_FACTOR = TRUE_H//IN_Hassert TRUE_W//IN_W == POOL_FACTORDATA_FOLDER  = "DataFolder/"FRAME_FOLDER = DATA_FOLDER + "Frames/"LOG_FOLDER   = DATA_FOLDER + "Logging/"EXAMPLES_FOLDER = DATA_FOLDER + "Examples/"CURR_VIDEO_NAME = "test_video"CURR_VIDEO_FRAMES_NAME = CURR_VIDEO_NAME + "_frames"LOAD_FRAME_DATA_VERBOSE = TrueFRAME_DIFF = 5TRAIN_ITERATIONS = 10000BATCH_SIZE = 64ACC_CUTOFF = 100BATCH_DIM = (BATCH_SIZE, *IN_DIM)def model_sanity_check(model):    x1 = torch.zeros(*BATCH_TEST_DIM).to(DEVICE)    x2 = torch.zeros(*BATCH_TEST_DIM).to(DEVICE)    model(x1, x2)    return Truedef load_frame_data(folderpath, verbose = False):    frames_list = sorted(os.listdir(folderpath))    num_frames  = len(frames_list)    all_frames  = np.zeros((num_frames, *IN_DIM))    for idx, frame in enumerate(frames_list):        filepath = folderpath + "/" + str(frame)        curr_frame = np.loadtxt(filepath)        curr_frame = np.reshape(curr_frame, TRUE_DIM)        for c in range(TRUE_C):            all_frames[idx][c] = AVG_POOL(curr_frame[c], POOL_FACTOR, POOL_METHOD)        if verbose and (idx+1)%100 == 0:            print("Retrieved " + str(idx+1) + " frames so far")    if verbose: print("Got all frames length: " + str(len(all_frames)))    all_frames = all_frames[500:-500]    if verbose: print("Stripped beginning and ending of frames")    return torch.from_numpy(all_frames).float().to(DEVICE)def gen_batch_sample(data, batch_size, frame_diff : int):    l = int(len(data))    b_idxs = torch.randint(0,l-frame_diff-frame_diff, size = (batch_size,))    a_idxs = b_idxs + frame_diff + frame_diff    t_idxs = b_idxs + frame_diff        before_batch = data[b_idxs]    after_batch  = data[a_idxs]    target_batch = data[t_idxs]        return (before_batch, after_batch, target_batch)def gen_batch_pure(data, frame_diff: int):    l = int(len(data))    b_idxs = torch.arange(l - frame_diff - frame_diff).long()    a_idxs = b_idxs + frame_diff + frame_diff    t_idxs = b_idxs + frame_diff        before_batch = data[b_idxs]    after_batch  = data[a_idxs]    target_batch = data[t_idxs]        return (before_batch, after_batch, target_batch)def log_vals(vals : dict):    for key, val in vals.items():        filepath = LOG_FOLDER + key + ".txt"        np.savetxt(filepath, np.array(val))        def log_models(fl, cl, gl):    torch.save(fl.state_dict(), LOG_FOLDER + "features_model_params.txt")    torch.save(cl.state_dict(), LOG_FOLDER + "concatention_model_params.txt")    torch.save(gl.state_dict(), LOG_FOLDER + "generation_model_params.txt")    def log_model(model):    torch.save(model.state_dict(), LOG_FOLDER + "model_params.txt")        def conv_image(image):    np_img = np.swapaxes(np.swapaxes(image.detach().numpy(), 0, 2), 0, 1)    return np_imgdef save_image(filepath, image):    np_img = conv_image(image)    plt.clf()    plt.imshow(np_img/255, interpolation='nearest')    plt.savefig(filepath)    plt.clf()        def save_images(folderpath, input_images, output_image):    if not(os.path.exists(folderpath)): os.mkdir(folderpath)    before_image, after_image, target_image = input_images        save_image(folderpath + "/before.png", before_image)    save_image(folderpath + "/after.png",  after_image)    save_image(folderpath + "/target.png", target_image)    save_image(folderpath + "/output.png",  output_image)    def save_examples(mode, model, batch):    assert mode == "val" or mode == "test" or mode == "train" or mode == "train_val"    loss_per = model.eval_loss_per(*batch)    idxs = torch.argsort(loss_per)    for i in range(5):        idx = idxs[i]        x_1, x_2, t_x = batch[0][idx], batch[1][idx], batch[2][idx]        out_x = model(x_1, x_2)[0]        folderpath = EXAMPLES_FOLDER + mode + "_best_" + str(i+1)        save_images(folderpath, (x_1, x_2, t_x), out_x)            for i in range(1,6):        idx = idxs[-i]        x_1, x_2, t_x = batch[0][idx], batch[1][idx], batch[2][idx]        out_x = model(x_1, x_2)[0]        folderpath = EXAMPLES_FOLDER + mode + "_worst_" + str(i)        save_images(folderpath, (x_1, x_2, t_x), out_x)    class FrameGenModel(nn.Module):    def __init__(self, in_height, in_width, feature_layers, combine_layers, generation_layers, loss_function, learning_rates: dict):        super().__init__()        self.in_h  = in_height        self.in_w  = in_width        self.loss_func = loss_function                self.featureModel = torch.nn.Sequential(*feature_layers)        self.combineModel = torch.nn.Sequential(*combine_layers)        self.generationModel = torch.nn.Sequential(*generation_layers)        self.skip_gen_layer  = len(generation_layers) == 1                self.feature_optimizer = torch.optim.Adam(self.featureModel.parameters(), lr=learning_rates['features'])        self.combine_optimizer = torch.optim.Adam(self.combineModel.parameters(), lr=learning_rates['combine'])        if not(self.skip_gen_layer): self.generation_optimizer = torch.optim.Adam(self.generationModel.parameters(), lr=learning_rates['generation'])            def make_batch(self, x):        if len(x.size()) == 3:            x = x.unsqueeze(0)        return x            def features_forward(self, in_x):        #print(in_x)        in_x = self.make_batch(in_x)        features_x = self.featureModel(in_x)        return features_x        def combine_forward(self, features_x):        features_x = self.make_batch(features_x)        combine_x = self.combineModel(features_x)        return combine_x        def generation_forward(self, combine_x):        combine_x = self.make_batch(combine_x)        image_out_x = self.generationModel(combine_x)        return image_out_x        def forward(self, in_x_1, in_x_2):        in_x_1 = self.make_batch(in_x_1)        in_x_2 = self.make_batch(in_x_2)        features_x_1  = self.features_forward(in_x_1)        features_x_2  = self.features_forward(in_x_2)        features_x    = torch.cat((features_x_1, features_x_2), dim = 1)        combine_x     = self.combine_forward(features_x)        image_out_x   = self.generation_forward(combine_x)        return image_out_x        def backprop(self, in_x_1, in_x_2, out_x):        in_x_1 = self.make_batch(in_x_1)        in_x_2 = self.make_batch(in_x_2)        out_x  = self.make_batch(out_x)                    self.feature_optimizer.zero_grad()        self.combine_optimizer.zero_grad()        if not(self.skip_gen_layer): self.generation_optimizer.zero_grad()                image_out_x = self.forward(in_x_1, in_x_2)        loss = self.loss_func(image_out_x, out_x)        loss.backward()                self.feature_optimizer.step()        self.combine_optimizer.step()        if not(self.skip_gen_layer): self.generation_optimizer.step()                return loss.item()        def eval_loss_total(self, in_x_1, in_x_2, out_x):        in_x_1 = self.make_batch(in_x_1)        in_x_2 = self.make_batch(in_x_2)        out_x  = self.make_batch(out_x)                image_out_x = self.forward(in_x_1, in_x_2)        loss = self.loss_func(image_out_x, out_x)                return loss.item()        def eval_loss_per(self, in_x_1, in_x_2, out_x):        in_x_1 = self.make_batch(in_x_1)        in_x_2 = self.make_batch(in_x_2)        out_x  = self.make_batch(out_x)                image_out_x = self.forward(in_x_1, in_x_2)        loss_per = torch.mean(torch.square(image_out_x - out_x), dim = (1,2,3))                if len(loss_per.size()) > 1: loss_per.squeeze(1)        assert len(loss_per.size()) == 1        return loss_per            def eval_acc(self, in_x_1, in_x_2, out_x, cutoff):        in_x_1 = self.make_batch(in_x_1)        in_x_2 = self.make_batch(in_x_2)        out_x  = self.make_batch(out_x)                loss_per = self.eval_loss_per(in_x_1, in_x_2, out_x)        acc_per  = torch.minimum(cutoff/loss_per, torch.zeros_like(loss_per) + 1)        return float(torch.mean(acc_per))        k_size = (5,5)in_h, in_w, feature_size = IN_H, IN_W, 32fl = [torch.nn.Conv2d(3, 32, k_size, padding = 'same'), torch.nn.MaxPool2d((2,2), stride = 2), torch.nn.BatchNorm2d(32), torch.nn.ReLU(),       torch.nn.Conv2d(32,16, k_size, padding = 'same'), torch.nn.BatchNorm2d(16), torch.nn.ReLU(),      torch.nn.Conv2d(16, 8, k_size, padding = 'same'), torch.nn.MaxPool2d((2,2), stride = 2), torch.nn.BatchNorm2d(8), torch.nn.ReLU(),      torch.nn.Conv2d(8, 4, k_size, padding = 'same'), torch.nn.BatchNorm2d(4), torch.nn.ReLU(),      torch.nn.Conv2d(4, 2, k_size, padding = 'same'), torch.nn.MaxPool2d((2,2), stride = 2), torch.nn.BatchNorm2d(2), torch.nn.ReLU(),      torch.nn.Conv2d(2, 1, k_size, padding = 'same'), torch.nn.BatchNorm2d(1), torch.nn.ReLU(),      torch.nn.Flatten()]cl = [torch.nn.Linear(2*4*4,128), torch.nn.ReLU(), torch.nn.Linear(128,32*32*3)]gl = [torch.nn.Unflatten(1, (3,IN_H,IN_W))]"""fl = [torch.nn.Conv2d(3, 2, k_size, padding = 'same'), torch.nn.MaxPool2d((2,2), stride = 2), torch.nn.ReLU(),       torch.nn.Conv2d(2, 1, k_size, padding = 'same'), torch.nn.MaxPool2d((2,2), stride = 2), torch.nn.ReLU(),      torch.nn.Flatten()]cl = [torch.nn.Linear(2*8*8,128), torch.nn.ReLU(), torch.nn.Linear(128,32*32*3)]gl = [torch.nn.Unflatten(1, (3,IN_H,IN_W))]"""learning_rates = {'features':1e-3, 'combine':1e-3, 'generation':1e-3}baselineModel = FrameGenModel(in_h, in_w, fl, cl, gl, torch.nn.functional.mse_loss, learning_rates).to(DEVICE)model_sanity_check(baselineModel)print("MODEL CREATED")baseline_video_data = data = load_frame_data(FRAME_FOLDER + CURR_VIDEO_FRAMES_NAME, verbose = LOAD_FRAME_DATA_VERBOSE)print("LOADED DATA")data_amt = int(len(baseline_video_data))train_amt = int(data_amt * 0.8)val_amt = int(data_amt * 0.1)test_amt = int(data_amt - train_amt - val_amt)train_data = data[:train_amt]val_data   = data[train_amt:train_amt + val_amt]test_data  = data[-test_amt:]in_train_batch = gen_batch_sample(train_data, BATCH_SIZE * 2, FRAME_DIFF)val_batch  = gen_batch_pure(val_data, FRAME_DIFF)test_batch = gen_batch_pure(test_data, FRAME_DIFF)print("CREATED TESTING AND VAL BATCHES")log = {'train_losses' : [], 'val_losses' : [], 'val_accuracies' : []}    for itr in range(TRAIN_ITERATIONS):    train_batch = gen_batch_sample(train_data, BATCH_SIZE, FRAME_DIFF)    train_loss  = baselineModel.backprop(*train_batch)    val_acc     = baselineModel.eval_acc(*val_batch, ACC_CUTOFF)    val_loss    = baselineModel.eval_loss_total(*val_batch)        log['train_losses'].append(train_loss)    log['val_losses'].append(val_loss)    log['val_accuracies'].append(val_acc)        log_vals(log)        if itr%10 == 0:        print("ITERATION: " + str(itr))        print("TRAIN LOSS: " + str(train_loss))        print("VAL ACCURACY: " + str(val_acc))        print("VAL LOSS: " + str(val_loss))        save_examples("val", baselineModel, val_batch)        save_examples("train_val", baselineModel, in_train_batch)        log_model(baselineModel)test_acc = baselineModel.eval_acc(*test_batch, ACC_CUTOFF)test_loss = baselineModel.eval_loss_total(*test_batch)print("LOSS ON TEST SET: " + str(test_loss))print("ACCURACY ON TEST SET: " + str(test_acc))save_examples("test", baselineModel, test_batch)